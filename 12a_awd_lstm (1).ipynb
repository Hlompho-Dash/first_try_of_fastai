{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"12a_awd_lstm.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jFtJjfW3dufG"},"source":["# AWD-LSTM"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZapiO85tdugd","executionInfo":{"status":"ok","timestamp":1618339443250,"user_tz":-120,"elapsed":6499,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}},"outputId":"42f70db2-99d7-4a58-9b9d-f98e7372bc8e"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":3,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLXUZJEQgBFi","executionInfo":{"status":"ok","timestamp":1618339496941,"user_tz":-120,"elapsed":60136,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}},"outputId":"095f50b3-8b73-494e-c267-c003984092ca"},"source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')\n","\n","print(\"------------------------------------------------------------------\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wnv0aDzhgCd6","executionInfo":{"status":"ok","timestamp":1618339602881,"user_tz":-120,"elapsed":166013,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}},"outputId":"042a176b-bf1a-4a8c-c87f-85d55b9a6aa1"},"source":["!git clone https://github.com/NVIDIA/apex\n","%cd apex\n","!pip install -v --no-cache-dir ./"],"execution_count":5,"outputs":[{"output_type":"stream","text":["fatal: destination path 'apex' already exists and is not an empty directory.\n","/content/gdrive/MyDrive/first_try_of_fastai/apex\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-bye0lb8p\n","Created temporary directory: /tmp/pip-req-tracker-vc6az1_s\n","Created requirements tracker '/tmp/pip-req-tracker-vc6az1_s'\n","Created temporary directory: /tmp/pip-install-isws1lv0\n","Processing /content/gdrive/MyDrive/first_try_of_fastai/apex\n","  Created temporary directory: /tmp/pip-req-build-4ttm9gxh\n","  Added file:///content/gdrive/MyDrive/first_try_of_fastai/apex to build tracker '/tmp/pip-req-tracker-vc6az1_s'\n","    Running setup.py (path:/tmp/pip-req-build-4ttm9gxh/setup.py) egg_info for package from file:///content/gdrive/MyDrive/first_try_of_fastai/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.8.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-4ttm9gxh/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-4ttm9gxh/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-4ttm9gxh/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-4ttm9gxh/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-4ttm9gxh/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-4ttm9gxh/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-4ttm9gxh/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-4ttm9gxh has version 0.1, which satisfies requirement apex==0.1 from file:///content/gdrive/MyDrive/first_try_of_fastai/apex\n","  Removed apex==0.1 from file:///content/gdrive/MyDrive/first_try_of_fastai/apex from build tracker '/tmp/pip-req-tracker-vc6az1_s'\n","Building wheels for collected packages: apex\n","  Created temporary directory: /tmp/pip-wheel-4i2pwifu\n","  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-4i2pwifu\n","  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-4ttm9gxh/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-4ttm9gxh/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-4i2pwifu --python-tag cp37\n","\n","\n","  torch.__version__  = 1.8.1+cu101\n","\n","\n","  /tmp/pip-req-build-4ttm9gxh/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  running bdist_wheel\n","  running build\n","  running build_py\n","  creating build\n","  creating build/lib\n","  creating build/lib/apex\n","  copying apex/__init__.py -> build/lib/apex\n","  creating build/lib/apex/RNN\n","  copying apex/RNN/cells.py -> build/lib/apex/RNN\n","  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n","  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n","  copying apex/RNN/models.py -> build/lib/apex/RNN\n","  creating build/lib/apex/reparameterization\n","  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n","  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n","  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n","  creating build/lib/apex/multi_tensor_apply\n","  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n","  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n","  creating build/lib/apex/normalization\n","  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n","  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n","  creating build/lib/apex/parallel\n","  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n","  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n","  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n","  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n","  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n","  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n","  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n","  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n","  creating build/lib/apex/optimizers\n","  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n","  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n","  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n","  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n","  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n","  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n","  creating build/lib/apex/mlp\n","  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n","  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n","  creating build/lib/apex/pyprof\n","  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n","  creating build/lib/apex/contrib\n","  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n","  creating build/lib/apex/amp\n","  copying apex/amp/handle.py -> build/lib/apex/amp\n","  copying apex/amp/_initialize.py -> build/lib/apex/amp\n","  copying apex/amp/__version__.py -> build/lib/apex/amp\n","  copying apex/amp/wrap.py -> build/lib/apex/amp\n","  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n","  copying apex/amp/opt.py -> build/lib/apex/amp\n","  copying apex/amp/utils.py -> build/lib/apex/amp\n","  copying apex/amp/scaler.py -> build/lib/apex/amp\n","  copying apex/amp/amp.py -> build/lib/apex/amp\n","  copying apex/amp/__init__.py -> build/lib/apex/amp\n","  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n","  copying apex/amp/frontend.py -> build/lib/apex/amp\n","  copying apex/amp/compat.py -> build/lib/apex/amp\n","  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n","  creating build/lib/apex/fp16_utils\n","  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n","  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n","  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n","  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n","  creating build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n","  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n","  creating build/lib/apex/pyprof/nvtx\n","  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n","  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n","  creating build/lib/apex/pyprof/parse\n","  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n","  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n","  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n","  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n","  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n","  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n","  creating build/lib/apex/contrib/groupbn\n","  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n","  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n","  creating build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n","  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n","  creating build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n","  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n","  creating build/lib/apex/contrib/layer_norm\n","  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n","  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n","  creating build/lib/apex/contrib/xentropy\n","  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n","  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n","  creating build/lib/apex/contrib/sparsity\n","  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n","  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n","  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n","  creating build/lib/apex/contrib/transducer\n","  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n","  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n","  creating build/lib/apex/amp/lists\n","  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n","  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n","  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n","  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n","  installing to build/bdist.linux-x86_64/wheel\n","  running install\n","  running install_lib\n","  creating build/bdist.linux-x86_64\n","  creating build/bdist.linux-x86_64/wheel\n","  creating build/bdist.linux-x86_64/wheel/apex\n","  creating build/bdist.linux-x86_64/wheel/apex/RNN\n","  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n","  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n","  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n","  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n","  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n","  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n","  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n","  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n","  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n","  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n","  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n","  creating build/bdist.linux-x86_64/wheel/apex/normalization\n","  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n","  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n","  creating build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n","  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n","  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n","  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n","  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n","  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n","  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n","  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n","  creating build/bdist.linux-x86_64/wheel/apex/mlp\n","  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n","  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n","  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n","  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n","  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n","  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n","  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n","  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n","  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n","  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n","  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n","  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n","  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n","  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n","  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n","  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n","  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n","  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n","  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n","  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n","  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n","  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n","  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n","  creating build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n","  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n","  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n","  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n","  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n","  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n","  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n","  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n","  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n","  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n","  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n","  running install_egg_info\n","  running egg_info\n","  creating apex.egg-info\n","  writing apex.egg-info/PKG-INFO\n","  writing dependency_links to apex.egg-info/dependency_links.txt\n","  writing top-level names to apex.egg-info/top_level.txt\n","  writing manifest file 'apex.egg-info/SOURCES.txt'\n","  writing manifest file 'apex.egg-info/SOURCES.txt'\n","  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n","  running install_scripts\n","  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n","  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n","  creating '/tmp/pip-wheel-4i2pwifu/apex-0.1-cp37-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n","  adding 'apex/__init__.py'\n","  adding 'apex/RNN/RNNBackend.py'\n","  adding 'apex/RNN/__init__.py'\n","  adding 'apex/RNN/cells.py'\n","  adding 'apex/RNN/models.py'\n","  adding 'apex/amp/__init__.py'\n","  adding 'apex/amp/__version__.py'\n","  adding 'apex/amp/_amp_state.py'\n","  adding 'apex/amp/_initialize.py'\n","  adding 'apex/amp/_process_optimizer.py'\n","  adding 'apex/amp/amp.py'\n","  adding 'apex/amp/compat.py'\n","  adding 'apex/amp/frontend.py'\n","  adding 'apex/amp/handle.py'\n","  adding 'apex/amp/opt.py'\n","  adding 'apex/amp/rnn_compat.py'\n","  adding 'apex/amp/scaler.py'\n","  adding 'apex/amp/utils.py'\n","  adding 'apex/amp/wrap.py'\n","  adding 'apex/amp/lists/__init__.py'\n","  adding 'apex/amp/lists/functional_overrides.py'\n","  adding 'apex/amp/lists/tensor_overrides.py'\n","  adding 'apex/amp/lists/torch_overrides.py'\n","  adding 'apex/contrib/__init__.py'\n","  adding 'apex/contrib/groupbn/__init__.py'\n","  adding 'apex/contrib/groupbn/batch_norm.py'\n","  adding 'apex/contrib/layer_norm/__init__.py'\n","  adding 'apex/contrib/layer_norm/layer_norm.py'\n","  adding 'apex/contrib/multihead_attn/__init__.py'\n","  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n","  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n","  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n","  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n","  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n","  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n","  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n","  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n","  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n","  adding 'apex/contrib/optimizers/__init__.py'\n","  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n","  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n","  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n","  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n","  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n","  adding 'apex/contrib/optimizers/fused_adam.py'\n","  adding 'apex/contrib/optimizers/fused_lamb.py'\n","  adding 'apex/contrib/optimizers/fused_sgd.py'\n","  adding 'apex/contrib/sparsity/__init__.py'\n","  adding 'apex/contrib/sparsity/asp.py'\n","  adding 'apex/contrib/sparsity/sparse_masklib.py'\n","  adding 'apex/contrib/transducer/__init__.py'\n","  adding 'apex/contrib/transducer/transducer.py'\n","  adding 'apex/contrib/xentropy/__init__.py'\n","  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n","  adding 'apex/fp16_utils/__init__.py'\n","  adding 'apex/fp16_utils/fp16_optimizer.py'\n","  adding 'apex/fp16_utils/fp16util.py'\n","  adding 'apex/fp16_utils/loss_scaler.py'\n","  adding 'apex/mlp/__init__.py'\n","  adding 'apex/mlp/mlp.py'\n","  adding 'apex/multi_tensor_apply/__init__.py'\n","  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n","  adding 'apex/normalization/__init__.py'\n","  adding 'apex/normalization/fused_layer_norm.py'\n","  adding 'apex/optimizers/__init__.py'\n","  adding 'apex/optimizers/fused_adagrad.py'\n","  adding 'apex/optimizers/fused_adam.py'\n","  adding 'apex/optimizers/fused_lamb.py'\n","  adding 'apex/optimizers/fused_novograd.py'\n","  adding 'apex/optimizers/fused_sgd.py'\n","  adding 'apex/parallel/LARC.py'\n","  adding 'apex/parallel/__init__.py'\n","  adding 'apex/parallel/distributed.py'\n","  adding 'apex/parallel/multiproc.py'\n","  adding 'apex/parallel/optimized_sync_batchnorm.py'\n","  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n","  adding 'apex/parallel/sync_batchnorm.py'\n","  adding 'apex/parallel/sync_batchnorm_kernel.py'\n","  adding 'apex/pyprof/__init__.py'\n","  adding 'apex/pyprof/nvtx/__init__.py'\n","  adding 'apex/pyprof/nvtx/nvmarker.py'\n","  adding 'apex/pyprof/parse/__init__.py'\n","  adding 'apex/pyprof/parse/__main__.py'\n","  adding 'apex/pyprof/parse/db.py'\n","  adding 'apex/pyprof/parse/kernel.py'\n","  adding 'apex/pyprof/parse/nvvp.py'\n","  adding 'apex/pyprof/parse/parse.py'\n","  adding 'apex/pyprof/prof/__init__.py'\n","  adding 'apex/pyprof/prof/__main__.py'\n","  adding 'apex/pyprof/prof/activation.py'\n","  adding 'apex/pyprof/prof/base.py'\n","  adding 'apex/pyprof/prof/blas.py'\n","  adding 'apex/pyprof/prof/conv.py'\n","  adding 'apex/pyprof/prof/convert.py'\n","  adding 'apex/pyprof/prof/data.py'\n","  adding 'apex/pyprof/prof/dropout.py'\n","  adding 'apex/pyprof/prof/embedding.py'\n","  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n","  adding 'apex/pyprof/prof/linear.py'\n","  adding 'apex/pyprof/prof/loss.py'\n","  adding 'apex/pyprof/prof/misc.py'\n","  adding 'apex/pyprof/prof/normalization.py'\n","  adding 'apex/pyprof/prof/optim.py'\n","  adding 'apex/pyprof/prof/output.py'\n","  adding 'apex/pyprof/prof/pointwise.py'\n","  adding 'apex/pyprof/prof/pooling.py'\n","  adding 'apex/pyprof/prof/prof.py'\n","  adding 'apex/pyprof/prof/randomSample.py'\n","  adding 'apex/pyprof/prof/recurrentCell.py'\n","  adding 'apex/pyprof/prof/reduction.py'\n","  adding 'apex/pyprof/prof/softmax.py'\n","  adding 'apex/pyprof/prof/usage.py'\n","  adding 'apex/pyprof/prof/utility.py'\n","  adding 'apex/reparameterization/__init__.py'\n","  adding 'apex/reparameterization/reparameterization.py'\n","  adding 'apex/reparameterization/weight_norm.py'\n","  adding 'apex-0.1.dist-info/LICENSE'\n","  adding 'apex-0.1.dist-info/METADATA'\n","  adding 'apex-0.1.dist-info/WHEEL'\n","  adding 'apex-0.1.dist-info/top_level.txt'\n","  adding 'apex-0.1.dist-info/RECORD'\n","  removing build/bdist.linux-x86_64/wheel\n","\u001b[?25hdone\n","  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=198540 sha256=3c3974d15a3d12744c582b52f06290104884a25b6b80f58fb87380bbd22da5a4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bye0lb8p/wheels/f4/d8/9d/d3a8bf6c7e2d179f7966d4d1ba6f1065d5a161ea99ba327898\n","  Removing source in /tmp/pip-req-build-4ttm9gxh\n","Successfully built apex\n","Installing collected packages: apex\n","\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-vc6az1_s'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DeitfY31dugn","executionInfo":{"status":"ok","timestamp":1618339626114,"user_tz":-120,"elapsed":189203,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","import os\n","os.chdir('/content/gdrive/MyDrive/first_try_of_fastai/exp')\n","from nb_12 import *\n","os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')\n","#from exp.nb_12 import *"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H3rYS1dBdugq"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"BAUUGCNLdugv"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6317)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BzmSUXZCdugy","executionInfo":{"status":"ok","timestamp":1618339652706,"user_tz":-120,"elapsed":215758,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}},"outputId":"b5834057-ff0c-451c-9755-9aa74213ffab"},"source":["path = datasets.untar_data(datasets.URLs.IMDB)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"fqNXKGkhdug0"},"source":["We have to preprocess the data again to pickle it because if we try to load the previous `SplitLabeledData` with pickle, it will complain some of the functions aren't in main."]},{"cell_type":"code","metadata":{"id":"bEmm2kbVdug6","executionInfo":{"status":"ok","timestamp":1618339652718,"user_tz":-120,"elapsed":215728,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["il = TextList.from_files(path, include=['train', 'test', 'unsup'])\n","sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPGF_OBudug_","executionInfo":{"status":"ok","timestamp":1618339652722,"user_tz":-120,"elapsed":215693,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3sx2MiMrduhD","executionInfo":{"status":"error","timestamp":1618339654378,"user_tz":-120,"elapsed":217313,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}},"outputId":"9f35cbed-581d-4139-eb87-050b4b65ddeb"},"source":["ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])"],"execution_count":10,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-796e760a6a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproc_tok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproc_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/gdrive/MyDrive/first_try_of_fastai/exp/nb_08.py\u001b[0m in \u001b[0;36mlabel_by_func\u001b[0;34m(sd, f, proc_x, proc_y)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlabel_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabeledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabeledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSplitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/first_try_of_fastai/exp/nb_08.py\u001b[0m in \u001b[0;36mlabel_by_func\u001b[0;34m(cls, il, f, proc_x, proc_y)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlabel_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_label_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlabel_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/first_try_of_fastai/exp/nb_08.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, proc_x, proc_y)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproc_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/first_try_of_fastai/exp/nb_08.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, il, proc)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m#   we'll discuss the changes in lesson 11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLabeledData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/first_try_of_fastai/exp/nb_08.py\u001b[0m in \u001b[0;36mcompose\u001b[0;34m(x, funcs, order_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_order\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuncs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/first_try_of_fastai/exp/nb_12.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mtoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"KsA3SD8tduhG","executionInfo":{"status":"aborted","timestamp":1618339652724,"user_tz":-120,"elapsed":215627,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n","pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fS-ojVDIduhI","executionInfo":{"status":"aborted","timestamp":1618339652727,"user_tz":-120,"elapsed":215595,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\n","vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bpllb6-CduhL","executionInfo":{"status":"aborted","timestamp":1618339654193,"user_tz":-120,"elapsed":217024,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["bs,bptt = 64,70\n","data = lm_databunchify(ll, bs, bptt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imG6YzeTduhN"},"source":["## AWD-LSTM"]},{"cell_type":"markdown","metadata":{"id":"7hY4pRYjduhO"},"source":["Before explaining what an AWD LSTM is, we need to start with an LSTM. RNNs were covered in part 1, if you need a refresher, there is a great visualization of them on [this website](http://joshvarty.github.io/VisualizingRNNs/)."]},{"cell_type":"markdown","metadata":{"id":"8KW3OvY7duhQ"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6330)"]},{"cell_type":"markdown","metadata":{"id":"s1UfWa3cduhS"},"source":["### LSTM from scratch"]},{"cell_type":"markdown","metadata":{"id":"pdM9P_SRduhU"},"source":["We need to implement those equations (where $\\sigma$ stands for sigmoid):\n","\n","![LSTM cell and equations](images/lstm.jpg)\n","(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)\n","\n","If we want to take advantage of our GPU, it's better to do one big matrix multiplication than four smaller ones. So we compute the values of the four gates all at once. Since there is a matrix multiplication and a bias, we use `nn.Linear` to do it.\n","\n","We need two linear layers: one for the input and one for the hidden state."]},{"cell_type":"code","metadata":{"id":"vV9QBGY0duhW","executionInfo":{"status":"aborted","timestamp":1618339654204,"user_tz":-120,"elapsed":216993,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["class LSTMCell(nn.Module):\n","    def __init__(self, ni, nh):\n","        super().__init__()\n","        self.ih = nn.Linear(ni,4*nh)\n","        self.hh = nn.Linear(nh,4*nh)\n","\n","    def forward(self, input, state):\n","        h,c = state\n","        #One big multiplication for all the gates is better than 4 smaller ones\n","        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n","        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n","        cellgate = gates[3].tanh()\n","\n","        c = (forgetgate*c) + (ingate*cellgate)\n","        h = outgate * c.tanh()\n","        return h, (h,c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2CEtjFOduhZ"},"source":["Then an LSTM layer just applies the cell on all the time steps in order."]},{"cell_type":"code","metadata":{"id":"Hzag-uYUduhb","executionInfo":{"status":"aborted","timestamp":1618339654209,"user_tz":-120,"elapsed":216965,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["class LSTMLayer(nn.Module):\n","    def __init__(self, cell, *cell_args):\n","        super().__init__()\n","        self.cell = cell(*cell_args)\n","\n","    def forward(self, input, state):\n","        inputs = input.unbind(1)\n","        outputs = []\n","        for i in range(len(inputs)):\n","            out, state = self.cell(inputs[i], state)\n","            outputs += [out]\n","        return torch.stack(outputs, dim=1), state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZTHreMkduhe"},"source":["Now let's try it out and see how fast we are. We only measure the forward pass."]},{"cell_type":"code","metadata":{"id":"Bjz23Ge8duhg","executionInfo":{"status":"aborted","timestamp":1618339654212,"user_tz":-120,"elapsed":216922,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["lstm = LSTMLayer(LSTMCell, 300, 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ou9qnJBduhh","executionInfo":{"status":"aborted","timestamp":1618339654214,"user_tz":-120,"elapsed":216886,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["x = torch.randn(64, 70, 300)\n","h = (torch.zeros(64, 300),torch.zeros(64, 300))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9dbYdZpduhj"},"source":["CPU"]},{"cell_type":"code","metadata":{"id":"MBomSD2aduhl","executionInfo":{"status":"aborted","timestamp":1618339654217,"user_tz":-120,"elapsed":216842,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["%timeit -n 10 y,h1 = lstm(x,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORbtfUtMduhn","executionInfo":{"status":"aborted","timestamp":1618339654220,"user_tz":-120,"elapsed":216762,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["lstm = lstm.cuda()\n","x = x.cuda()\n","h = (h[0].cuda(), h[1].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NsDN7Tdduhp","executionInfo":{"status":"aborted","timestamp":1618339654222,"user_tz":-120,"elapsed":216720,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["def time_fn(f):\n","    f()\n","    torch.cuda.synchronize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDkoaSXdduhr"},"source":["CUDA"]},{"cell_type":"code","metadata":{"id":"CsCmDl_Lduhs","executionInfo":{"status":"aborted","timestamp":1618339654225,"user_tz":-120,"elapsed":216691,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["f = partial(lstm,x,h)\n","time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"idjLtKOsduhu","executionInfo":{"status":"aborted","timestamp":1618339654229,"user_tz":-120,"elapsed":216646,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["%timeit -n 10 time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"92RDg2Pdduhv"},"source":["### Builtin version"]},{"cell_type":"markdown","metadata":{"id":"u3FTAgmdduhx"},"source":["Let's compare with PyTorch!"]},{"cell_type":"markdown","metadata":{"id":"kcT-EEHNduhz"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6718)"]},{"cell_type":"code","metadata":{"id":"2X5TNFuuduh3","executionInfo":{"status":"aborted","timestamp":1618339654233,"user_tz":-120,"elapsed":216610,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["lstm = nn.LSTM(300, 300, 1, batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqTHL5h_duh5","executionInfo":{"status":"aborted","timestamp":1618339654236,"user_tz":-120,"elapsed":216575,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["x = torch.randn(64, 70, 300)\n","h = (torch.zeros(1, 64, 300),torch.zeros(1, 64, 300))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-8Ap4QPduh7"},"source":["CPU"]},{"cell_type":"code","metadata":{"id":"NOWOlGIXduh9","executionInfo":{"status":"aborted","timestamp":1618339654238,"user_tz":-120,"elapsed":216544,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["%timeit -n 10 y,h1 = lstm(x,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYZHT7rTduh-","executionInfo":{"status":"aborted","timestamp":1618339654240,"user_tz":-120,"elapsed":216515,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["lstm = lstm.cuda()\n","x = x.cuda()\n","h = (h[0].cuda(), h[1].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3QY0h35duiA","executionInfo":{"status":"aborted","timestamp":1618339654242,"user_tz":-120,"elapsed":216484,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["f = partial(lstm,x,h)\n","time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dPNPsKcduiB"},"source":["GPU"]},{"cell_type":"code","metadata":{"id":"Tx75jT1YduiD","executionInfo":{"status":"aborted","timestamp":1618339654244,"user_tz":-120,"elapsed":216442,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["%timeit -n 10 time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XPaGac4duiE"},"source":["So our version is running at almost the same speed on the CPU. However, on the GPU, PyTorch uses CuDNN behind the scenes that optimizes greatly the for loop."]},{"cell_type":"markdown","metadata":{"id":"TMYD7kTIduiH"},"source":["### Jit version"]},{"cell_type":"markdown","metadata":{"id":"BpMADBmrduiK"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=6744)"]},{"cell_type":"code","metadata":{"id":"VxPjXS7jduiM","executionInfo":{"status":"aborted","timestamp":1618339654245,"user_tz":-120,"elapsed":216410,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["import torch.jit as jit\n","from torch import Tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSeaZsr-duiP"},"source":["We have to write everything from scratch to be a bit faster, so we don't use the linear layers here."]},{"cell_type":"code","metadata":{"id":"GKW8eBt5duiQ","executionInfo":{"status":"aborted","timestamp":1618339654249,"user_tz":-120,"elapsed":216385,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["class LSTMCell(jit.ScriptModule):\n","    def __init__(self, ni, nh):\n","        super().__init__()\n","        self.ni = ni\n","        self.nh = nh\n","        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))\n","        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))\n","        self.bias_ih = nn.Parameter(torch.randn(4 * nh))\n","        self.bias_hh = nn.Parameter(torch.randn(4 * nh))\n","\n","    @jit.script_method\n","    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n","        hx, cx = state\n","        gates = (input @ self.w_ih.t() + self.bias_ih +\n","                 hx @ self.w_hh.t() + self.bias_hh)\n","        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n","\n","        ingate = torch.sigmoid(ingate)\n","        forgetgate = torch.sigmoid(forgetgate)\n","        cellgate = torch.tanh(cellgate)\n","        outgate = torch.sigmoid(outgate)\n","\n","        cy = (forgetgate * cx) + (ingate * cellgate)\n","        hy = outgate * torch.tanh(cy)\n","\n","        return hy, (hy, cy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVjJxonyduiU","executionInfo":{"status":"aborted","timestamp":1618339654252,"user_tz":-120,"elapsed":216356,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["class LSTMLayer(jit.ScriptModule):\n","    def __init__(self, cell, *cell_args):\n","        super().__init__()\n","        self.cell = cell(*cell_args)\n","\n","    @jit.script_method\n","    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n","        inputs = input.unbind(1)\n","        outputs = []\n","        for i in range(len(inputs)):\n","            out, state = self.cell(inputs[i], state)\n","            outputs += [out]\n","        return torch.stack(outputs, dim=1), state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S23Q10EwduiX","executionInfo":{"status":"aborted","timestamp":1618339654254,"user_tz":-120,"elapsed":216320,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["lstm = LSTMLayer(LSTMCell, 300, 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERwUpXHOduiZ","executionInfo":{"status":"aborted","timestamp":1618339654256,"user_tz":-120,"elapsed":216290,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["x = torch.randn(64, 70, 300)\n","h = (torch.zeros(64, 300),torch.zeros(64, 300))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tgFh3QvUduib","executionInfo":{"status":"aborted","timestamp":1618339654266,"user_tz":-120,"elapsed":216268,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["%timeit -n 10 y,h1 = lstm(x,h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzvZUBV4duic","executionInfo":{"status":"aborted","timestamp":1618339654269,"user_tz":-120,"elapsed":216235,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["lstm = lstm.cuda()\n","x = x.cuda()\n","h = (h[0].cuda(), h[1].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzygvYkuduie","executionInfo":{"status":"aborted","timestamp":1618339654270,"user_tz":-120,"elapsed":216200,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["f = partial(lstm,x,h)\n","time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iGb6LT-duif","executionInfo":{"status":"aborted","timestamp":1618339654273,"user_tz":-120,"elapsed":216163,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["%timeit -n 10 time_fn(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZfdvjZXLduig"},"source":["With jit, we almost get to the CuDNN speed!"]},{"cell_type":"markdown","metadata":{"id":"TzcLem_Wduig"},"source":["### Dropout"]},{"cell_type":"markdown","metadata":{"id":"cFpHOdUAduih"},"source":["We want to use the AWD-LSTM from [Stephen Merity et al.](https://arxiv.org/abs/1708.02182). First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor `1/(1-p)` (think of what happens to the activations if you want to figure out why!)\n","\n","We usually apply dropout by drawing a mask that tells us which elements to nullify or not:"]},{"cell_type":"markdown","metadata":{"id":"F-x809e4duij"},"source":["[Jump_to lesson 12 video](https://course19.fast.ai/videos/?lesson=12&t=7003)"]},{"cell_type":"code","metadata":{"id":"524bUdGNduik","executionInfo":{"status":"aborted","timestamp":1618339654275,"user_tz":-120,"elapsed":216131,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","def dropout_mask(x, sz, p):\n","    return x.new(*sz).bernoulli_(1-p).div_(1-p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cc5rgKXBduil","executionInfo":{"status":"aborted","timestamp":1618339654277,"user_tz":-120,"elapsed":216095,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["x = torch.randn(10,10)\n","mask = dropout_mask(x, (10,10), 0.5); mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9oqGlkSRduil"},"source":["Once with have a dropout mask `mask`, applying the dropout to `x` is simply done by `x = x * mask`. We create our own dropout mask and don't rely on pytorch dropout because we do not want to nullify all the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension."]},{"cell_type":"code","metadata":{"id":"AhI0EYpYduim","executionInfo":{"status":"aborted","timestamp":1618339654281,"user_tz":-120,"elapsed":216063,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["(x*mask).std(),x.std()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jl13pFfduin"},"source":["Inside a RNN, a tensor x will have three dimensions: bs, seq_len, vocab_size.  Recall that we want to consistently apply the dropout mask across the seq_len dimension, therefore, we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension."]},{"cell_type":"code","metadata":{"id":"Ot12e-BDduiq","executionInfo":{"status":"aborted","timestamp":1618339654283,"user_tz":-120,"elapsed":216034,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class RNNDropout(nn.Module):\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","        self.p=p\n","\n","    def forward(self, x):\n","        if not self.training or self.p == 0.: return x\n","        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n","        return x * m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSDV69dDduir","executionInfo":{"status":"aborted","timestamp":1618339654285,"user_tz":-120,"elapsed":215997,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["dp = RNNDropout(0.3)\n","tst_input = torch.randn(3,3,7)\n","tst_input, dp(tst_input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bhjsz9T6duis"},"source":["WeightDropout is dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CuDNN speed and not reimplement the cell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass."]},{"cell_type":"code","metadata":{"id":"gBcI06Shduit","executionInfo":{"status":"aborted","timestamp":1618339654287,"user_tz":-120,"elapsed":215963,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","import warnings\n","\n","WEIGHT_HH = 'weight_hh_l0'\n","\n","class WeightDropout(nn.Module):\n","    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n","        super().__init__()\n","        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n","        for layer in self.layer_names:\n","            #Makes a copy of the weights of the selected layers.\n","            w = getattr(self.module, layer)\n","            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n","            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n","\n","    def _setweights(self):\n","        for layer in self.layer_names:\n","            raw_w = getattr(self, f'{layer}_raw')\n","            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n","\n","    def forward(self, *args):\n","        self._setweights()\n","        with warnings.catch_warnings():\n","            #To avoid the warning that comes because the weights aren't flattened.\n","            warnings.simplefilter(\"ignore\")\n","            return self.module.forward(*args)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYNyCQrrduix"},"source":["Let's try it!"]},{"cell_type":"code","metadata":{"id":"bTMEui3Qduiz","executionInfo":{"status":"aborted","timestamp":1618339654289,"user_tz":-120,"elapsed":215914,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["module = nn.LSTM(5, 2)\n","dp_module = WeightDropout(module, 0.4)\n","getattr(dp_module.module, WEIGHT_HH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"70wzcl2Adui0"},"source":["It's at the beginning of a forward pass that the dropout is applied to the weights."]},{"cell_type":"code","metadata":{"id":"v_cIDs8Ndui1","executionInfo":{"status":"aborted","timestamp":1618339654321,"user_tz":-120,"elapsed":215902,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["tst_input = torch.randn(4,20,5)\n","h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n","x,h = dp_module(tst_input,h)\n","getattr(dp_module.module, WEIGHT_HH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"go7Xmpoqdui3"},"source":["EmbeddingDropout applies dropout to full rows of the embedding matrix."]},{"cell_type":"code","metadata":{"id":"UmbEGpm1dui4","executionInfo":{"status":"aborted","timestamp":1618339654323,"user_tz":-120,"elapsed":215854,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class EmbeddingDropout(nn.Module):\n","    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n","    def __init__(self, emb, embed_p):\n","        super().__init__()\n","        self.emb,self.embed_p = emb,embed_p\n","        self.pad_idx = self.emb.padding_idx\n","        if self.pad_idx is None: self.pad_idx = -1\n","\n","    def forward(self, words, scale=None):\n","        if self.training and self.embed_p != 0:\n","            size = (self.emb.weight.size(0),1)\n","            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n","            masked_embed = self.emb.weight * mask\n","        else: masked_embed = self.emb.weight\n","        if scale: masked_embed.mul_(scale)\n","        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n","                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9C2mSP3sdui7","executionInfo":{"status":"aborted","timestamp":1618339654325,"user_tz":-120,"elapsed":215804,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["enc = nn.Embedding(100, 7, padding_idx=1)\n","enc_dp = EmbeddingDropout(enc, 0.5)\n","tst_input = torch.randint(0,100,(8,))\n","enc_dp(tst_input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFonebe8dui8"},"source":["### Main model"]},{"cell_type":"markdown","metadata":{"id":"wpy4JPCYdui9"},"source":["The main model is a regular LSTM with several layers, but using all those kinds of dropouts."]},{"cell_type":"code","metadata":{"id":"WL-M3mnJdui_","executionInfo":{"status":"aborted","timestamp":1618339654327,"user_tz":-120,"elapsed":215745,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","def to_detach(h):\n","    \"Detaches `h` from its history.\"\n","    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYSlMVzxdujA","executionInfo":{"status":"aborted","timestamp":1618339654329,"user_tz":-120,"elapsed":215693,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class AWD_LSTM(nn.Module):\n","    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n","    initrange=0.1\n","\n","    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n","                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n","        super().__init__()\n","        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n","        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n","        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n","        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n","                             batch_first=True) for l in range(n_layers)]\n","        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n","        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n","        self.input_dp = RNNDropout(input_p)\n","        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n","\n","    def forward(self, input):\n","        bs,sl = input.size()\n","        if bs!=self.bs:\n","            self.bs=bs\n","            self.reset()\n","        raw_output = self.input_dp(self.emb_dp(input))\n","        new_hidden,raw_outputs,outputs = [],[],[]\n","        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n","            raw_output, new_h = rnn(raw_output, self.hidden[l])\n","            new_hidden.append(new_h)\n","            raw_outputs.append(raw_output)\n","            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n","            outputs.append(raw_output) \n","        self.hidden = to_detach(new_hidden)\n","        return raw_outputs, outputs\n","\n","    def _one_hidden(self, l):\n","        \"Return one hidden state.\"\n","        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n","        return next(self.parameters()).new(1, self.bs, nh).zero_()\n","\n","    def reset(self):\n","        \"Reset the hidden states.\"\n","        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FjfHIsIbdujE"},"source":["On top of this, we will apply a linear decoder. It's often best to use the same matrix as the one for the embeddings in the weights of the decoder."]},{"cell_type":"code","metadata":{"id":"Bjp2HGnMdujI","executionInfo":{"status":"aborted","timestamp":1618339654330,"user_tz":-120,"elapsed":215659,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class LinearDecoder(nn.Module):\n","    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n","        super().__init__()\n","        self.output_dp = RNNDropout(output_p)\n","        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n","        if bias: self.decoder.bias.data.zero_()\n","        if tie_encoder: self.decoder.weight = tie_encoder.weight\n","        else: init.kaiming_uniform_(self.decoder.weight)\n","\n","    def forward(self, input):\n","        raw_outputs, outputs = input\n","        output = self.output_dp(outputs[-1]).contiguous()\n","        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n","        return decoded, raw_outputs, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrQrMwPLdujK","executionInfo":{"status":"aborted","timestamp":1618339654332,"user_tz":-120,"elapsed":215622,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class SequentialRNN(nn.Sequential):\n","    \"A sequential module that passes the reset call to its children.\"\n","    def reset(self):\n","        for c in self.children():\n","            if hasattr(c, 'reset'): c.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C09gcctfdujM"},"source":["And now we stack all of this together!"]},{"cell_type":"code","metadata":{"id":"ZJCfJmHtdujN","executionInfo":{"status":"aborted","timestamp":1618339654334,"user_tz":-120,"elapsed":215588,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n","                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n","    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n","                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n","    enc = rnn_enc.emb if tie_weights else None\n","    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EO69pnL1dujP","executionInfo":{"status":"aborted","timestamp":1618339654336,"user_tz":-120,"elapsed":215557,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["tok_pad = vocab.index(PAD)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSWuEv9mdujQ"},"source":["Now we can test this all works without throwing a bug."]},{"cell_type":"code","metadata":{"id":"EyXiDk54dujS","executionInfo":{"status":"aborted","timestamp":1618339654337,"user_tz":-120,"elapsed":215525,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad)\n","tst_model = tst_model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6ckfyzDdujT","executionInfo":{"status":"aborted","timestamp":1618339654339,"user_tz":-120,"elapsed":215496,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["x,y = next(iter(data.train_dl))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7uV011WdujU","executionInfo":{"status":"aborted","timestamp":1618339654342,"user_tz":-120,"elapsed":215466,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["z = tst_model(x.cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yzc6qjcjdujW"},"source":["We return three things to help with regularization: the true output (probabilities for each word), but also the activations of the encoder, with or without dropouts."]},{"cell_type":"code","metadata":{"id":"2cVCdJWYdujX","executionInfo":{"status":"aborted","timestamp":1618339654345,"user_tz":-120,"elapsed":215430,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["len(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36DnD48fdujY","executionInfo":{"status":"aborted","timestamp":1618339654349,"user_tz":-120,"elapsed":215398,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["decoded, raw_outputs, outputs = z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQh5TBtuduja"},"source":["The decoded tensor is flattened to `bs * seq_len` by `len(vocab)`:"]},{"cell_type":"code","metadata":{"id":"GidkG2lqdujb","executionInfo":{"status":"aborted","timestamp":1618339654350,"user_tz":-120,"elapsed":215362,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["decoded.size()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OR93uOvidujc"},"source":["`raw_outputs` and `outputs` each contain the results of the intermediary layers:"]},{"cell_type":"code","metadata":{"id":"Rz3s6h93duje","executionInfo":{"status":"aborted","timestamp":1618339654352,"user_tz":-120,"elapsed":215324,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["len(raw_outputs),len(outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqOclDV4dujf","executionInfo":{"status":"aborted","timestamp":1618339654354,"user_tz":-120,"elapsed":215291,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["[o.size() for o in raw_outputs], [o.size() for o in outputs]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IpTA7ZJwdujg"},"source":["### Callbacks to train the model"]},{"cell_type":"markdown","metadata":{"id":"_bhp1zXGdujh"},"source":["We need to add a few tweaks to train a language model: first we will clip the gradients. This is a classic technique that will allow us to use a higher learning rate by putting a maximum value on the norm of the gradients."]},{"cell_type":"code","metadata":{"id":"mLJBqXlwdujj","executionInfo":{"status":"aborted","timestamp":1618339654356,"user_tz":-120,"elapsed":215261,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class GradientClipping(Callback):\n","    def __init__(self, clip=None): self.clip = clip\n","    def after_backward(self):\n","        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJupw_Nydujk"},"source":["Then we add an `RNNTrainer` that will do four things:\n","- change the output to make it contain only the `decoded` tensor (for the loss function) and store the `raw_outputs` and `outputs`\n","- apply Activation Regularization (AR): we add to the loss an L2 penalty on the last activations of the AWD LSTM (with dropout applied)\n","- apply Temporal Activation Regularization (TAR): we add to the loss an L2 penalty on the difference between two consecutive (in terms of words) raw outputs\n","- trigger the shuffle of the LMDataset at the beginning of each epoch"]},{"cell_type":"code","metadata":{"id":"q5ZtpuJ9dujm","executionInfo":{"status":"aborted","timestamp":1618339654358,"user_tz":-120,"elapsed":215229,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","class RNNTrainer(Callback):\n","    def __init__(self, α, β): self.α,self.β = α,β\n","    \n","    def after_pred(self):\n","        #Save the extra outputs for later and only returns the true output.\n","        self.raw_out,self.out = self.pred[1],self.pred[2]\n","        self.run.pred = self.pred[0]\n","    \n","    def after_loss(self):\n","        #AR and TAR\n","        if self.α != 0.:  self.run.loss += self.α * self.out[-1].float().pow(2).mean()\n","        if self.β != 0.:\n","            h = self.raw_out[-1]\n","            if h.size(1)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n","                \n","    def begin_epoch(self):\n","        #Shuffle the texts at the beginning of the epoch\n","        if hasattr(self.dl.dataset, \"batchify\"): self.dl.dataset.batchify()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MggMWwJ6dujn"},"source":["Lastly we write a flattened version of the cross entropy loss and the accuracy metric."]},{"cell_type":"code","metadata":{"id":"b_qev6PTdujp","executionInfo":{"status":"aborted","timestamp":1618339654364,"user_tz":-120,"elapsed":215202,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["#export\n","def cross_entropy_flat(input, target):\n","    bs,sl = target.size()\n","    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))\n","\n","def accuracy_flat(input, target):\n","    bs,sl = target.size()\n","    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WI5GqRohdujq","executionInfo":{"status":"aborted","timestamp":1618339654366,"user_tz":-120,"elapsed":215159,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["emb_sz, nh, nl = 300, 300, 2\n","model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, \n","                           embed_p=0.1, hidden_p=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blOrbJqIdujs","executionInfo":{"status":"aborted","timestamp":1618339654368,"user_tz":-120,"elapsed":215131,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["cbs = [partial(AvgStatsCallback,accuracy_flat),\n","       CudaCallback, Recorder,\n","       partial(GradientClipping, clip=0.1),\n","       partial(RNNTrainer, α=2., β=1.),\n","       ProgressCallback]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccFf5wR9dujt","executionInfo":{"status":"aborted","timestamp":1618339654369,"user_tz":-120,"elapsed":215098,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoUI1PpMduju","executionInfo":{"status":"aborted","timestamp":1618339654371,"user_tz":-120,"elapsed":215061,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["learn.fit(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojRldn6ndujv"},"source":["## Export"]},{"cell_type":"code","metadata":{"id":"JyxiChy4dujw","executionInfo":{"status":"aborted","timestamp":1618339654373,"user_tz":-120,"elapsed":215029,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":["!python notebook2script.py 12a_awd_lstm.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kepnEQOlduj0","executionInfo":{"status":"aborted","timestamp":1618339654375,"user_tz":-120,"elapsed":214995,"user":{"displayName":"Hlompho Lekaka","photoUrl":"","userId":"06191567976139515257"}}},"source":[""],"execution_count":null,"outputs":[]}]}