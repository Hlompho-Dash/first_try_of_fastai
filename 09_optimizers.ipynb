{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7WOnvkXlXujYekBnrB0+r"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s06sZAWCz6b1",
        "outputId": "0377226e-093e-4623-a607-293851010a92"
      },
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')\n",
        "\n",
        "print(\"------------------------------------------------------------------\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIJlCdpH2bHx"
      },
      "source": [
        "#export\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/first_try_of_fastai/exp')\n",
        "from nb_08 import *\n",
        "os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY0ItqKc2dNx",
        "outputId": "59ff9fc7-044f-4cb5-b170-d01e9397fb1e"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl6ikYvf7qv_"
      },
      "source": [
        "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3cmUuwV9PVF"
      },
      "source": [
        "import PIL,os,mimetypes\n",
        "Path.ls = lambda x: list(x.iterdir())\n",
        "\n",
        "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/') )\n",
        "\n",
        "def setify(o): return o if isinstance(o,set) else set(listify(o))\n",
        "\n",
        "def _get_files(p, fs, extensions=None):\n",
        "  p = Path(p)\n",
        "  res = [p/f for f in fs if not f.startswith('.')\n",
        "          and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in  extensions)]\n",
        "  return res\n",
        "\n",
        "def get_files(path, extensions=None, recurse=False, include=None):\n",
        "  path = Path(path)\n",
        "  extensions = setify(extensions)\n",
        "  extensions = {e.lower() for e in extensions}\n",
        "  if recurse:\n",
        "    res = []\n",
        "    for p,d,f in os.walk(path):\n",
        "      if include is not None: d[:] = [o for o in d if o in include]\n",
        "      else:                   d[:] = [o for o in d if not o.startswith('.')]\n",
        "      res += _get_files(p,f,extensions)\n",
        "    return res\n",
        "  else:\n",
        "    f = [o.name for o in os.scandir(path) if o.is_file()]\n",
        "    return _get_files(path, f, extensions)\n",
        "\n",
        "def compose(x, funcs, *args, order_key=\"_order\", **kwargs):\n",
        "  key = lambda o: getattr(o, order_key, 0)\n",
        "  for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
        "  return x\n",
        "\n",
        "class ItemList(ListContainer):\n",
        "  def __init__(self, items, path='.', tfms=None):\n",
        "    super().__init__(items)\n",
        "    self.path, self.tfms = Path(path), tfms\n",
        "\n",
        "  def __repr__(self): return f'{super().__repr__()}\\nPath: {self.path}'\n",
        "  def new(self, items): return self.__class__(items, self.path, tfms = self.tfms)\n",
        "\n",
        "  def get(self, i): return i\n",
        "  def _get(self, i): return compose(self.get(i), self.tfms)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    res = super().__getitem__(idx)\n",
        "    if isinstance(res, list): return [self._get(o) for o in res]\n",
        "    return self._get(res)\n",
        "\n",
        "class ImageList(ItemList):\n",
        "  @classmethod\n",
        "  def from_files(cls, path, extensions = None, recurse=True, include=None, **kwargs):\n",
        "    if extensions is None: extensions = image_extensions\n",
        "    return cls(get_files(path, extensions, recurse=recurse, include = include), path, **kwargs)\n",
        "\n",
        "  def get(self, fn): return PIL.Image.open(fn)\n",
        "\n",
        "class Transform(): _order=0\n",
        "\n",
        "class MakeRGB(Transform):\n",
        "  def __call__(self, item): return item.convert('RGB')\n",
        "\n",
        "def make_rgb(item): return item.convert('RGB')\n",
        "\n",
        "def grandparent_splitter(fn, valid_name = 'valid', train_name=\"train\"):\n",
        "  gp = fn.parent.parent.name\n",
        "  return True if gp==valid_name else False if gp==train_name else None\n",
        "\n",
        "def split_by_func(ds, f):\n",
        "  items = ds.items\n",
        "  mask = [f(o) for o in items]\n",
        "\n",
        "  train = [o for o,m in zip(items, mask) if m==False]\n",
        "  valid = [o for o,m in zip(items, mask) if m==True]\n",
        "  return train, valid\n",
        "\n",
        "class SplitData():\n",
        "  def __init__(self, train, valid): self.train, self.valid = train, valid\n",
        "\n",
        "  def __getattr__(self,k): return getattr(self.train,k)\n",
        "\n",
        "  @classmethod\n",
        "  def split_by_func(cls, il, f):\n",
        "    lists = map(il.new, split_by_func(il, f))\n",
        "    return cls(*lists)\n",
        "\n",
        "  def __repr__(self): return f'{self.__class__.__name__}\\nTrain: {self.train}\\nValid: {self.valid}\\n'\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "def uniqueify(x, sort = False):\n",
        "  res = list(OrderedDict.fromkeys(x).keys())\n",
        "  if sort: res.sort()\n",
        "  return res\n",
        "\n",
        "class Processor():\n",
        "  def process(self, items): return items\n",
        "\n",
        "class CategoryProcessor(Processor):\n",
        "  def __init__(self): self.vocab=None\n",
        "\n",
        "  def process(self, items):\n",
        "    if self.vocab is None:\n",
        "      self.vocab = uniqueify(items)\n",
        "      self.otoi = {v:k for k,v in enumerate(self.vocab)}\n",
        "    return [self.proc1(o) for o in items]\n",
        "\n",
        "  def proc1(self, item): return self.otoi[item]\n",
        "\n",
        "  def deprocess(self, idxs):\n",
        "    assert self.vocab is not None\n",
        "    return [self.deproc1(idx) for idx in idxs]\n",
        "\n",
        "  def deproc1(self, idx): return self.vocab[idx]\n",
        "\n",
        "class ProcessedItemList(ListContainer):\n",
        "  def __init__(self, inputs, processor):\n",
        "    self.processor = processor\n",
        "    items = processor.process(inputs)\n",
        "    super().__init__(items)\n",
        "\n",
        "  def obj(self, idx):\n",
        "    res = self[idx]\n",
        "    if isinstance(res, (tuple, list, Generator)): return self.processor.deprocess(res)\n",
        "    return self.processor.deproc1(idx)\n",
        "\n",
        "def parent_labeler(fn): return fn.parent.name\n",
        "\n",
        "def _label_by_func(ds, f): return [f(o) for o in ds.items]\n",
        "\n",
        "class LabeledData():\n",
        "  def __init__(self, x ,y): self.x, self.y = x,y\n",
        "\n",
        "  def __repr__(self): return f'{self.__class__.__name__}\\nx: {self.x}\\ny:{self.y}\\n'\n",
        "  def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
        "  def __len__(self): return len(self.x)\n",
        "  \n",
        "  @classmethod\n",
        "  def _label_by_func(cls, il, f, proc=None):\n",
        "    labels = _label_by_func(il, f)\n",
        "    proc_labels = ProcessedItemList(labels, proc)\n",
        "    return cls(il, proc_labels)\n",
        "\n",
        "def label_by_func(sd, f):\n",
        "  proc = CategoryProcessor()\n",
        "  train = LabeledData._label_by_func(sd.train, f, proc)\n",
        "  valid = LabeledData._label_by_func(sd.valid, f, proc)\n",
        "  return SplitData(train, valid)\n",
        "\n",
        "class ResizeFixed(Transform):\n",
        "  _order = 10\n",
        "  def __init__(self, size):\n",
        "    if isinstance(size, int): size = (size, size)\n",
        "    self.size = size\n",
        "\n",
        "  def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)\n",
        "\n",
        "def to_byte_tensor(item):\n",
        "  res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))\n",
        "  w,h = item.size\n",
        "  return res.view(h,w,-1).permute(2,0,1)\n",
        "to_byte_tensor._order = 20\n",
        "\n",
        "def to_float_tensor(item): return item.float().div_(255.)\n",
        "to_float_tensor._order = 30\n",
        "\n",
        "def show_image(im, figsize=(3,3)):\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(im.permute(1,2,0))\n",
        "\n",
        "class DataBunch():\n",
        "  def __init__(self, train_dl, valid_dl, c_in = None, c_out=None):\n",
        "    self.train_dl, self.valid_dl, self.c_in, self.c_out = train_dl,valid_dl,c_in,c_out\n",
        "\n",
        "  @property\n",
        "  def train_ds(self): return self.train_dl.dataset\n",
        "\n",
        "  @property\n",
        "  def valid_ds(self): return self.valid_dl.dataset\n",
        "\n",
        "def databunchify(sd, bs, c_in=None, c_out=None, **kwargs):\n",
        "  dls = get_dls(sd.train, sd.valid, bs, **kwargs)\n",
        "  return DataBunch(*dls, c_in=c_in, c_out=c_out)\n",
        "\n",
        "SplitData.to_databunch = databunchify\n",
        "\n",
        "def normalize_chan(x, mean, std):\n",
        "  return (x-mean[...,None,None]) / std[...,None,None]\n",
        "\n",
        "_m = tensor([0.47, 0.48, 0.45])\n",
        "_s = tensor([0.29, 0.28, 0.30])\n",
        "\n",
        "norm_imagenette = partial(normalize_chan, mean= _m.cuda(), std=_s.cuda())\n",
        "\n",
        "import math\n",
        "def prev_pow_2(x): return 2**math.floor(math.log2(x))\n",
        "\n",
        "def get_cnn_layers(data, nfs, layer, **kwargs):\n",
        "  def f(ni,nf,stride=2): return layer(ni, nf,3,stride=stride, **kwargs)\n",
        "  print(data.c_in)\n",
        "  l1 = data.c_in\n",
        "  l2 = prev_pow_2(l1*3*3)\n",
        "  print(2**math.floor(math.log2(l1*3*3)))\n",
        "  print(prev_pow_2(27))\n",
        "  print(l1)\n",
        "  layers = [f(l1 , l2 , stride=1 ),\n",
        "            f(l2 , l2*2 , stride=2),\n",
        "            f(l2*2, l2*4 , stride=2)]\n",
        "  nfs = [l2*4] + nfs\n",
        "  layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)]\n",
        "  layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten),\n",
        "             nn.Linear(nfs[-1], data.c_out)]\n",
        "  return layers\n",
        "\n",
        "def get_cnn_model(data, nfs, layer, **kwargs):\n",
        "  return nn.Sequential(*get_cnn_layers(data,nfs,layer,**kwargs))\n",
        "\n",
        "def get_learn_run(nfs, data, lr, layer, cbs=None,opt_func=None, **kwargs):\n",
        "  model = get_cnn_model(data, nfs, layer, **kwargs)\n",
        "  init_cnn(model)\n",
        "  return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)\n",
        "\n",
        "def model_summary(run, learn, find_all=False):\n",
        "  xb,yb = get_batch(data.valid_dl, run)\n",
        "  device = next(learn.model.parameters()).device\n",
        "  xb,yb = xb.to(device),yb.to(device)\n",
        "  mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()\n",
        "  f = lambda hook,mod,inp,out: print(f'{mod}\\n{out.shape}\\n')\n",
        "  with Hooks(mods, f) as hooks: learn.model(xb)\n",
        "\n",
        "def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n",
        "  layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias= not bn),\n",
        "            GeneralRelu(**kwargs)]\n",
        "  if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))\n",
        "  return nn.Sequential(*layers)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paB0BD0172LV",
        "outputId": "a5a4e859-e02e-4eba-bac9-2812451944bb"
      },
      "source": [
        "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\n",
        "bs = 128\n",
        "\n",
        "il = ImageList.from_files(path, tfms=tfms)\n",
        "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\n",
        "ll = label_by_func(sd, parent_labeler)\n",
        "data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSEU8e1B9AqG"
      },
      "source": [
        "nfs = [32,64,128,256]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCwADF4qDK2z"
      },
      "source": [
        "cbfs = [partial(AvgStatsCallback, accuracy), CudaCallback,\n",
        "        partial(BatchTransformXCallback, norm_imagenette)]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "komFPOocDauA",
        "outputId": "e4f12185-fd41-4ac9-a3a2-4c20023ca3af"
      },
      "source": [
        "learn, run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "16\n",
            "16\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPW_zKB4DiFC",
        "outputId": "f762e81c-350c-4a48-9da9-6fe8e8288b64"
      },
      "source": [
        "run.fit(1, learn)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train: [1.7737068823265392, tensor(0.3898, device='cuda:0')]\n",
            "valid: [1.6310045145504937, tensor(0.4611, device='cuda:0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3GHhnj3Mc8D"
      },
      "source": [
        "**refininf the optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OboR2fQIMOph"
      },
      "source": [
        "class Optimizer():\n",
        "  def __init__(self, params, steppers, **defaults):\n",
        "    self.param_groups = list(params)\n",
        "    if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
        "    self.hypers = [{**defaults} for p in self.param_groups]\n",
        "    self.steppers = listify(steppers)\n",
        "\n",
        "  def grad_params(self):\n",
        "    return [(p,hyper) for pg,hyper in zip(self.param_groups, self.hypers)\n",
        "            for p in pg if p.grad is not None]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p,hyper in self.grad_params():\n",
        "      p.grad.detach_()\n",
        "      p.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    for p,hyper  in self.grad_params()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}