
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/09_optimizers.ipynb
import os
os.chdir('/content/gdrive/MyDrive/first_try_of_fastai/exp')
from nb_08 import *
os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')

def sgd_step(p, lr, **kwargs):
  p.data.add_(-lr, p.grad.data)
  return p

class Recorder(Callback):
  def begin_fit(self): self.lrs,self.losses = [], []

  def after_batch(self):
    #if not in_train: return THIS in_train line was causing errors in the code
    self.lrs.append(self.opt.hypers[-1]['lr'])
    self.losses.append(self.loss.detach().cpu())

  def plot_lr(self): plt.plot(self.lrs)
  def plot_loss(self): plt.plot(self.losses)

class ParamScheduler(Callback):
  _order = 1
  def __init__(self, pname, sched_func):
    self.pname, self.sched_func = pname, sched_func

  def set_param(self):
    for h in self.opt.hypers:
      h[self.pname] = self.sched_func(self.n_epochs/ self.epochs)

  def begin_batch(self):
    if self.in_train: self.set_param()

def weight_decay(p, lr, wd, **kwargs):
  p.data.mul_(1 - lr*wd)
  return p

weight_decay._defaults = dict(wd=0.)

def l2_reg(p, lr, wd, **kwargs):
  p.grad.data.add_(wd, p.data)
  return p
l2_reg._defaults = dict(wd=0.)

def maybe_update(os, dest, f):
  for o in os:
    for k,v in f(o).items():
      if k not in dest: dest[k] = v

def get_defaults(d): return getattr(d, "_defaults", {})

sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])

class StatefulOptimizer(Optimizer):
  def __init__(self, params, steppers, stats=None, **defaults):
    self.stats = listify(stats)
    maybe_update(self.stats, defaults, get_defaults)
    super().__init__(params, steppers, **defaults)
    self.state = {}

  def step(self):
    for p, hyper in self.grad_params():
      if p not in self.state:
        self.state[p] = {}
        maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))
      state = self.state[p]
      for stat in self.stats: state = stat.update(p, state, **hyper)
      compose(p, self.steppers, **state, **hyper)
      self.state[p] = state

class Stat():
  _defaults = {}
  def init_state(self, p): raise NotImplementedError
  def update(self, p, state, **kwargs): raise NotImplementedError

def momentum_step(p , lr, grad_avg, **kwargs):
  p.data.add_(-lr, grad_avg)
  return p