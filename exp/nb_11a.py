
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/11a_train_imagennete.ipynb
import os
os.chdir('/content/gdrive/MyDrive/first_try_of_fastai/exp')
from nb_11 import *
os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')

def cos_1cycle_anneal(start, high, end):
    return [sched_cos(start, high), sched_cos(high, end)]

def create_phases(phases):
    phases = listify(phases)
    return phases + [1-sum(phases)]

def noop(x): return x

class Flatten(nn.Module):
  def forward(self, x): return x.view(x.size(0), -1)

act_fn = nn.ReLU(inplace=True)

def init_cnn(m):
  if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
  if isinstance(m, (nn.Conv2d, nn.Linear)): nn.init.kaiming_normal_(m.weight)
  for l in m.children(): init_cnn(l)

def cnn_learner(arch, data, loss_func,opt_func, c_in=None, c_out=None,
                lr=1e-2, cuda=True, norm=None, progress=True, mixup=0,xtra_cb=None, **kwargs):
  cbfs = [partial(AvgStatsCallback,accuracy)] + listify(xtra_cb)
  if progress: cbfs.append(ProgressCallback)
  if cuda: cbfs.append(CudaCallback)
  if norm: cbfs.append(partial(BatchTransformXCallback, norm))
  if mixup: cbfs.append(partial(MixUp, mixup))
  arch_args = {}
  if not c_in: c_in = data.c_in
  if not c_out: c_out = data.c_out
  if c_in: arch_args['c_in'] = c_in
  if c_out: arch_args['c_out'] = c_out
  return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)

class CategoryProcessor(Processor):
    def __init__(self): self.vocab=None

    def __call__(self, items):
        #The vocab is defined on the first use.
        if self.vocab is None:
            self.vocab = uniqueify(items)
            self.otoi  = {v:k for k,v in enumerate(self.vocab)}
        return [self.proc1(o) for o in items]
    def proc1(self, item):  return self.otoi[item]

    def deprocess(self, idxs):
        assert self.vocab is not None
        return [self.deproc1(idx) for idx in idxs]
    def deproc1(self, idx): return self.vocab[idx]

def xresnet18 (**kwargs): return XResNet.create(1, [2,2,2 ,2], **kwargs)
def xresnet34 (**kwargs): return XResNet.create(1, [3,4,6 ,3], **kwargs)
def xresnet50 (**kwargs): return XResNet.create(4, [3,4,6 ,3], **kwargs)
def xresnet101(**kwargs): return XResNet.create(4, [3,4,23,3], **kwargs)
def xresnet152(**kwargs): return XResNet.create(4, [3,8,36,3], **kwargs)

class XResNet(nn.Sequential):
  @classmethod
  def create(cls, expansion, layers, c_in=3, c_out=1000):
    nfs = [c_in, (c_in+1)*8, 64, 64]
    stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==8 else 1)
            for i in range(3)]

    nfs = [64//expansion, 64, 128, 256, 512]
    res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],
                                 n_blocks=1, stride=1 if i==0 else 2)
                  for i,l in enumerate(layers)]

    res = cls(
        *stem,
        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        *res_layers,
        nn.AdaptiveAvgPool2d(1), Flatten(),
        nn.Linear(nfs[-1]*expansion, c_out),
    )

    init_cnn(res)
    return res

  @staticmethod
  def _make_layer(expansion, ni, nf, n_blocks, stride):
      return nn.Sequential(
          *[ResBlock(expansion, ni if i== 0 else nf, nf, stride if i==0 else 1)
          for i in range(n_blocks)])

def conv(ni, nf, ks=3, stride=1, bias = False):
  return nn.Conv2d(ni, nf, kernel_size = ks, stride=stride, padding=ks//2, bias =bias)

def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):
  bn = nn.BatchNorm2d(nf)
  nn.init.constant_(bn.weight, 0. if zero_bn else 1.)
  layers = [conv(ni, nf, ks, stride=stride), bn]
  if act: layers.append(act_fn)
  return nn.Sequential(*layers)

class ResBlock(nn.Module):
  def __init__(self, expansion, ni , nh, stride = 1):
    super().__init__()
    nf,ni = nh*expansion,ni*expansion
    layers =  [conv_layer(ni, nh, 1)]
    layers += [
        conv_layer(nh, nf, 3, stride=stride, zero_bn=True, act=False)
    ] if expansion==1 else [
        conv_layer(nh, nh, 3, stride=stride),
        conv_layer(nh, nf, 1, zero_bn = True, act=False)
    ]
    self.convs = nn.Sequential(*layers)
    self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)
    self.pool = noop if stride==1 else nn.AvgPool2d(2)

  def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))

class GeneralRelu(nn.Module):
    def __init__(self, leak=None, sub=None, maxv=None):
        super().__init__()
        self.leak,self.sub,self.maxv = leak,sub,maxv

def random_splitter(fn, p_valid): return random.random() < p_valid

class AdaptiveConcatPool2d(nn.Module):
  def __init__(self, sz=1):
    super().__init__()
    self.output_size = sz
    self.ap = nn.AdaptiveAvgPool2d(sz)
    self.mp = nn.AdaptiveMaxPool2d(sz)
  def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)

from types import SimpleNamespace
cb_types = SimpleNamespace(**{o:o for o in Learner.ALL_CBS})

class DebugCallback(Callback):
  _order = 999
  def __init__(self, cb_name, f=None): self.cb_name, self.f = cb_name, f
  def __call__(self, cn_name):
    if cb_name == self.cb_name:
      if self.f: self.f(self.run)
      else: set_trace()

def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0,95):
  phases = create_phases(pct_start)
  sched_lr = [combine_sched(phases, cos_1cycle_anneal(lr.10., lr, lr/1e5))
              for lr in lrs]
  sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))
  return [ParamScheduler('lr', sched_lr),
          Paramscheduler('mom', sched_mom)]