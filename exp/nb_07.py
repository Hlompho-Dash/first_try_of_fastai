
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/07_batchNorm.ipynb
import os
os.chdir('/content/gdrive/MyDrive/first_try_of_fastai/exp')
from nb_06 import *
os.chdir('/content/gdrive/MyDrive/first_try_of_fastai')

def init_cnn_(m,f):
  if isinstance(m, nn.Conv2d):
    f(m.weight, a=0.1)
    if getattr(m, 'bias',None) is not None: m.bias.data.zero_()
  for l in m.children(): init_cnn_(l, f)

def init_cnn(m, uniform=False):
  f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
  init_cnn_(m,f)

def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
  model = get_cnn_model(data, nfs, layer, **kwargs)
  init_cnn(model, uniform=uniform)
  return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)

def append_stats(i, mod, inp, outp):
  act_means[i].append(outp.data.mean())
  act_stds [i].append(outp.data.std())

def children(m): return list(m.children())

class Hook():
  def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
  def remove(self): self.hook.remove()
  def __del__(self): self.remove()

def append_stats(hook, mod, inp, outp):
  if not hasattr(hook,'stats'): hook.stats = ([],[])
  means, stds = hook.stats
  means.append(outp.data.mean())
  stds.append(outp.data.std())

class ParamScheduler(Callback):
  _order = 1
  def __init__(self, pname, sched_func):
    self.pname, self.sched_func = pname, sched_func

  def set_param(self):
    for pg in self.opt.param_groups:
      pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)

  def begin_batch(self):
    if self.in_train: self.set_param()

def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
  layers = [nn.Conv2d(ni,nf,ks,padding=ks//2,stride=stride,bias=not bn),
            GeneralRelu(**kwargs)]
  if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
  return nn.Sequential(*layers)